## https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# The following variables are supported:
#
# AIRFLOW_UID                  - User ID in Airflow containers
#                                Default: 50000
# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.
#                                Default: .
# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
#
# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
#                                Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
#                                Default: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
#                                Use this option ONLY for quick checks. Installing requirements at container
#                                startup is done EVERY TIME the service is started.
#                                A better way is to build a custom image or extend the official image
#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.
#                                Default: ''

---
x-airflow-common: &airflow-common
  # In order to add custom dependencies or upgrade provider distributions you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  build:
    context: .
    dockerfile: airflow/Dockerfile
  env_file:
    - .env
  environment: &airflow-common-env
    LANG: en_US.UTF-8
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-db/${AIRFLOW_DB}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-db/${AIRFLOW_DB}
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-apiserver:8080/execution/"
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
    AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
    AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    AWS_REGION: europe-central-1

    # The following line can be used to set a custom config file, stored in the local config folder
    AIRFLOW_CONFIG: "/opt/airflow/config/airflow.cfg"

  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/config:/opt/airflow/config
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/scripts:/opt/airflow/scripts
  user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    airflow-db:
      condition: service_healthy
    minio:
      condition: service_healthy
    spark-master:
      condition: service_healthy
    spark-worker:
      condition: service_healthy
  networks:
    - all_in_one

x-spark-common: &spark-common
  build:
    context: .
    dockerfile: jupyter-spark/jupyter-spark-config/Dockerfile
  env_file:
    - .env
  environment: &spark-common-env
    SPARK_NO_DAEMONIZE: true
    AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
    AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    AWS_REGION: europe-central-1
  volumes:
    - ./jupyter-spark/spark-logs:/opt/spark/logs
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/scripts:/opt/airflow/scripts
  networks:
    - all_in_one

services:
  source-db:
    build:
      context: .
      dockerfile: data/databases/source_db/Dockerfile
    environment:
      POSTGRES_USER: ${SOURCE_DB_USER}
      POSTGRES_PASSWORD: ${SOURCE_DB_PASSWORD}
      POSTGRES_DB: ${SOURCE_DB}
    env_file:
      - .env
    ports:
      - "5444:5432"
    volumes:
      - source-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${SOURCE_DB_USER}"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - all_in_one

  airflow-db:
    image: postgres:16
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB}
    env_file:
      - .env
    ports:
      - "5442:5432"
    volumes:
      - airflow-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${AIRFLOW_DB_USER}"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - all_in_one

  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always
    networks:
      - all_in_one

  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-apiserver:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command: >
      -c "
        chmod +x /opt/airflow/scripts/airflow_init.sh
        /opt/airflow/scripts/airflow_init.sh
      "

    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD}
      _PIP_ADDITIONAL_REQUIREMENTS: ""
    user: "0:0"

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow
    depends_on:
      <<: *airflow-common-depends-on

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: minio
    restart: always
    ports:
      - "9001:9001" # UI
      - "9009:9000" # API
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    env_file:
      - .env
    volumes:
      - ./minio_storage:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - all_in_one

  create-bucket:
    image: minio/mc
    entrypoint: >
      /bin/sh -c "
      until mc alias set local http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; do
        echo 'Waiting for MinIO...';
        sleep 5;
      done;

      mc mb -p local/${MINIO_RAW_BUCKET_NAME} || true;
      mc policy set public local/${MINIO_RAW_BUCKET_NAME} || true;

      mc mb -p local/${MINIO_MART_BUCKET_NAME} || true;
      mc policy set public local/${MINIO_MART_BUCKET_NAME} || true;

      mc mb -p local/${MINIO_DEV_BUCKET_NAME} || true;
      mc policy set public local/${MINIO_DEV_BUCKET_NAME} || true;
      "
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - all_in_one

  spark-master:
    <<: *spark-common
    environment:
      <<: *spark-common-env
      SPARK_MODE: master
    ports:
      - "7077:7077" # Spark master
      - "18080:8080" # Spark master UI
    restart: always
    entrypoint: >
      /bin/sh -c "
        /opt/spark/sbin/start-master.sh &&
        tail -f /dev/null
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  spark-worker:
    <<: *spark-common
    environment:
      <<: *spark-common-env
      SPARK_MASTER_URL: spark://spark-master:7077
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8081:8081" # Worker UI
    restart: always
    entrypoint: >
      /bin/sh -c "
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /dev/null
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  jupyter-spark:
    <<: *spark-common
    environment:
      <<: *spark-common-env
      SPARK_MASTER_URL: spark://spark-master:7077
    ports:
      - "16000:8888"
      - "4040:4040"
    volumes:
      - ./jupyter-spark/jupyter-spark-workspace/dev:/workspace/dev
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker:
        condition: service_healthy
    restart: always
    entrypoint: >
      /bin/sh -c "
      jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --ServerApp.token=${NOTEBOOK_TOKEN}
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 25s
    profiles:
      - dev

  iceberg-db:
    image: postgres:16
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${ICEBERG_USER}
      POSTGRES_PASSWORD: ${ICEBERG_PASSWORD}
      POSTGRES_DB: ${ICEBERG_DB}
    ports:
      - "9432:5432"
    volumes:
      - iceberg-db-volume:/var/lib/postgresql/data
    healthcheck:
      test:
        ["CMD", "pg_isready", "-U", "${ICEBERG_USER}", "-d", "${ICEBERG_DB}"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - all_in_one

  rest:
    image: tabulario/iceberg-rest:1.6.0
    ports:
      - "8181:8181"
    env_file:
      - .env
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: eu-central-1
      CATALOG_WAREHOUSE: s3://${MINIO_RAW_BUCKET_NAME}/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: true
      CATALOG_URI: jdbc:postgresql://iceberg-db:5432/${ICEBERG_DB}?user=${ICEBERG_USER}&password=${ICEBERG_PASSWORD}
      CATALOG_JDBC_DRIVER: org.postgresql.Driver
      CATALOG_JDBC_USER: ${ICEBERG_USER}
      CATALOG_JDBC_PASSWORD: ${ICEBERG_PASSWORD}
    depends_on:
      iceberg-db:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: always
    networks:
      - all_in_one

  trino:
    container_name: trino
    image: trinodb/trino:478
    env_file:
      - .env
    environment:
      TRINO_USER: ${TRINO_USER}
      TRINO_PASSWORD: ${TRINO_PASSWORD}
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: eu-central-1
      S3_USE_PATH_STYLE_ACCESS: true
      AWS_ENDPOINT_URL: http://minio:9000
      AWS_S3_DISABLE_SSL: true
      SOURCE_DB_USER: ${SOURCE_DB_USER}
      SOURCE_DB_PASSWORD: ${SOURCE_DB_PASSWORD}
      CONNECTION_URL: jdbc:postgresql://source-db:5432/${SOURCE_DB}
      # JAVA_TOOL_OPTIONS: "-Dtrino.authentication.type=BASIC -Dtrino.password-file.path=/etc/trino/password.db"
    volumes:
      - ./trino-config:/etc/trino
    ports:
      - "8086:8080"
    depends_on:
      minio:
        condition: service_healthy
      rest:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: always
    networks:
      - all_in_one

  dbt:
    build:
      context: .
      dockerfile: dbt/Dockerfile
    depends_on:
      trino:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./dbt:/usr/app/dbt
    working_dir: /usr/app/dbt/dbt_pipeline
    command: >
      sh -c "
        dbt docs generate --profiles-dir /usr/app/dbt/dbt_pipeline/profiles &&
        dbt docs serve --host 0.0.0.0 --port 8091 --profiles-dir /usr/app/dbt/dbt_pipeline/profiles
      "
    ports:
      - "8091:8091"
    restart: always
    environment:
      TRINO_USER: ${TRINO_USER}
      TRINO_PASSWORD: ${TRINO_PASSWORD}
      TRINO_HOST: trino
      TRINO_PORT: 8080
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "dbt debug --profiles-dir /usr/app/dbt/dbt_pipeline/profiles",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - all_in_one

volumes:
  source-db-volume:
  airflow-db-volume:
  iceberg-db-volume:
networks:
  all_in_one:
    driver: bridge
